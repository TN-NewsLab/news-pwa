import os
import re
import requests
import feedparser
import json
import sys
sys.stdout.reconfigure(encoding='utf-8')
sys.stderr.reconfigure(encoding='utf-8')

from openai import OpenAI
from dotenv import load_dotenv
from datetime import datetime, timezone, timedelta
from pathlib import Path

REPO_ROOT = Path(__file__).resolve().parents[1]
DATA_PATH = REPO_ROOT / "docs" / "data" / "summary_v2.json"

DATA_PATH.parent.mkdir(parents=True, exist_ok=True)

# 1) .env èª­ã¿è¾¼ã¿
load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=API_KEY)

RSS_SOURCES = {
    "ai": {
        "source": "VentureBeat",
        "url": "https://venturebeat.com/feed/"
    },
    "economy": {
        "source": "NHK",
        "url": "https://www3.nhk.or.jp/rss/news/cat5.xml"
    },
    "world": {
        "source": "BBC",
        "url": "https://feeds.bbci.co.uk/news/world/rss.xml"
    },
    "japan_politics": {
        "source": "NHK",
        "url": "https://www3.nhk.or.jp/rss/news/cat3.xml"
    }
}

# ********** RSSå–å¾— **********
def fetch_rss(url):
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    }

    resp = requests.get(url, headers=headers, timeout=10)
    resp.raise_for_status()

    feed = feedparser.parse(resp.content)
    entries = feed.entries
        
    # è¨˜äº‹ãŒç©ºã§ãªã„æ™‚ã ã‘1ä»¶
    return feed.entries[0] if feed.entries else None

def fetch_rss_ai_multiple(url, max_items=2):
    """
    AIã‚«ãƒ†ã‚´ãƒªå°‚ç”¨ï¼šOpenAI/ChatGPTé–¢é€£ã‚’å„ªå…ˆã—ã¤ã¤ã€
    æœ€å¤§ max_items ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¿”ã™é–¢æ•°ã€‚
    """
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    }

    resp = requests.get(url, headers=headers, timeout=10)
    resp.raise_for_status()

    feed = feedparser.parse(resp.content)
    entries = feed.entries

    if not entries:
        return []

    keywords = ["openai", "chatgpt", "sam altman", "gpt", "large language model"]

    # --- â‘  å„ªå…ˆè¨˜äº‹ï¼ˆOpenAI/ChatGPTé–¢é€£ï¼‰ã‚’å…ˆã«å–å¾—
    priority_items = []
    normal_items = []

    for e in entries:
        text = (e.title + " " + e.get("summary", "")).lower()
        if any(k in text for k in keywords):
            priority_items.append(e)
        else:
            normal_items.append(e)

    # --- â‘¡ å„ªå…ˆ â†’ é€šå¸¸ ã®é †ã§ max_items ä»¶å–ã‚Šå‡ºã™
    combined = priority_items + normal_items
    return combined[:max_items]

# ********** titleãƒ»summaryæŠ½å‡º / 3è¡Œè¦ç´„ **********
def summarize(text, title=""):
    date_rule = (
            "è¦ç´„ã§ã¯å¹´å·ï¼ˆä¾‹ï¼š2023å¹´ã€2025å¹´ãªã©ï¼‰ã‚’ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚"
            "æ—¥ä»˜ãŒå¿…è¦ãªå ´åˆã¯ã€Œä»Šå¹´ã€ã€Œæœ€è¿‘ã€ã€Œ9æœˆæœ«æ™‚ç‚¹ã€ãªã©ã®ç›¸å¯¾è¡¨ç¾ã®ã¿ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚"
    )
 
    if not text or text.strip() == "":
        prompt = (
            "æ¬¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã€è¨˜äº‹ã®å†…å®¹ã‚’æ¨æ¸¬ã—ã¦3è¡Œã®è¦ç´„æ–‡ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n"
            f"{date_rule}\n"
            f"ã‚¿ã‚¤ãƒˆãƒ«: {title}\n"
        )
    else:
            prompt = (
                f"{date_rule}\n"
                f"æ¬¡ã®è¨˜äº‹ã‚’3è¡Œã§è¦ç´„ã—ã¦ãã ã•ã„ï¼š\n{text}"
            )
    # else:
    #      prompt = f"æ¬¡ã®è¨˜äº‹ã‚’3è¡Œã§è¦ç´„ã—ã¦ãã ã•ã„ï¼š\n{text}"
    
    res = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )

    content = res.choices[0].message.content

    # content ãŒæ–‡å­—åˆ—ã®å ´åˆ
    if isinstance(content, str):
        return content

    # content ãŒé…åˆ—ï¼ˆMessageContentï¼‰ã§è¿”ã‚‹å ´åˆï¼ˆå°†æ¥ã®ä»•æ§˜å¤‰æ›´å¯¾ç­–ï¼‰
    if isinstance(content, list) and len(content) > 0:
        first = content[0]
        # textå±æ€§ã‚’æŒã¤ã‚¿ã‚¤ãƒ—
        if hasattr(first, "text"):
            return first.text
        # ä¸‡ãŒä¸€ text ãŒãªãã¦ã‚‚ string_value ãŒã‚ã‚‹
        if hasattr(first, "string_value"):
            return first.string_value

    # ãã‚Œã§ã‚‚ãƒ€ãƒ¡ãªã‚‰ã€ã¨ã‚Šã‚ãˆãšæ–‡å­—åˆ—åŒ–ã—ã¦è¿”ã™
    return str(content)

# ********** è‹±èªã‚¿ã‚¤ãƒˆãƒ«ï¼†è¦ç´„ â†’ æ—¥æœ¬èªç¿»è¨³ **********
def translate_to_japanese(title_en: str, summary_en: str):
    """
    VentureBeat / BBC ãªã©è‹±èªè¨˜äº‹å°‚ç”¨ã€‚
    ã‚¿ã‚¤ãƒˆãƒ«ã¨è¦ç´„ã‚’ã¾ã¨ã‚ã¦æ—¥æœ¬èªãƒ‹ãƒ¥ãƒ¼ã‚¹æ–‡ä½“ã«ç¿»è¨³ã™ã‚‹ã€‚
    ã†ã¾ããƒ‘ãƒ¼ã‚¹ã§ããªã‘ã‚Œã°ã€å…ƒã®è‹±èªã‚’ãã®ã¾ã¾è¿”ã™ã€‚
    """
    system_prompt = (
        "You are a professional Japanese news editor.\n"
        "Translate the provided English title and summary into clear, natural Japanese "
        "suitable for news readers. Preserve meaning strictly, avoid embellishment, "
        "and maintain factual accuracy.\n"
        "Return the result as JSON with keys: title_ja, summary_ja."
    )

    user_prompt = f"""
Translate the following text into Japanese.

Title:
{title_en}

Summary:
{summary_en}
""".strip()

    res = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    )

    content = res.choices[0].message.content

    if not isinstance(content, str):
        content = str(content)

    text = content.strip()

    # ```json ï½ ``` ã§è¿”ã£ã¦ããŸå ´åˆã®ã‚±ã‚¢
    if text.startswith("```"):
        lines = text.splitlines()
        # ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹è¡Œã‚’å‰Šã‚‹
        lines = [l for l in lines if not l.strip().startswith("```")]
        text = "\n".join(lines).strip()

    title_ja = title_en
    summary_ja = summary_en

    try:
        data = json.loads(text)
        t = data.get("title_ja")
        s = data.get("summary_ja")
        if isinstance(t, str) and t.strip():
            title_ja = t.strip()
        if isinstance(s, str) and s.strip():
            summary_ja = s.strip()
    except Exception as e:
        print("âš ï¸ ç¿»è¨³çµæœã®JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—:", e)
        print("  è¿”å´ãƒ†ã‚­ã‚¹ãƒˆ:", text[:200], "...")

    return title_ja, summary_ja

# ********** ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šï¼ˆAI / çµŒæ¸ˆ / ãã®ä»–ï¼‰ **********
import re

def classify_category(title: str, summary: str, initial_category: str, description: str = "") -> str:
    key = (initial_category or "").strip().lower()

    text = f"{title} {description} {summary}"
    t = text.lower()

    # --- AIãƒãƒ¼ã‚«ãƒ¼ï¼ˆsaidç­‰ã®èª¤æ¤œå‡ºæŠ‘åˆ¶ï¼‰---
    has_ai_marker = bool(re.search(r"(?i)(?<![a-z])ai(?![a-z])", text)) or ("äººå·¥çŸ¥èƒ½" in text)

    # --- çµŒæ¸ˆï¼šå¼·ã„é‡‘èãƒ»ç›¸å ´èªï¼ˆã“ã“ã«å½“ãŸã£ãŸæ™‚ã ã‘çµŒæ¸ˆã«ã™ã‚‹ï¼‰---
    finance_strong_keywords = [
        # JPï¼ˆå¼·ã„ï¼‰
        "æ ª", "æ ªä¾¡", "éŠ˜æŸ„", "æ±ºç®—", "æ¥­ç¸¾", "æŠ•è³‡", "ç›¸å ´", "ãƒãƒ–ãƒ«",
        "æ€¥é¨°", "æš´è½", "åç™º", "èª¿æ•´",
        "é‡‘åˆ©", "åˆ©ä¸Šã’", "åˆ©ä¸‹ã’", "å›½å‚µ", "å‚µåˆ¸", "åˆ©å›ã‚Š",
        "ç‚ºæ›¿", "å††é«˜", "å††å®‰",
        "æ—¥çµŒ", "æ—¥çµŒå¹³å‡", "topix", "ãƒ€ã‚¦", "ãƒŠã‚¹ãƒ€ãƒƒã‚¯", "s&p",
        "æ™‚ä¾¡ç·é¡", "ipo", "etf",

        # ENï¼ˆå¼·ã„ï¼‰
        "stock", "stocks", "share", "shares", "equity", "earnings", "revenue",
        "invest", "investor", "fund", "ipo", "etf", "valuation",
        "interest rate", "bond", "treasury", "yield",
        "nasdaq", "dow", "s&p", "nikkei", "topix", "forex", "fx",
    ]

    # â€»ã€Œä¼æ¥­ã€ã€Œè³‡é‡‘ã€ã¿ãŸã„ãªæ±ç”¨èªã¯å…¥ã‚Œãªã„ï¼ˆèª¤çˆ†ã™ã‚‹ã‹ã‚‰ï¼‰
    finance_hits = sum(1 for k in finance_strong_keywords if k in t)

    # --- AIï¼šAæ–¹é‡ï¼ˆç”ŸæˆAI/LLMä¸­å¿ƒ + AIã‚¤ãƒ³ãƒ•ãƒ©ï¼‰ ---
    ai_core_keywords = [
        "chatgpt", "openai", "gpt", "llm", "large language model",
        "anthropic", "claude", "gemini", "deepseek",
        "ç”Ÿæˆai", "å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«", "åŸºç›¤ãƒ¢ãƒ‡ãƒ«",
        "diffusion", "stable diffusion", "midjourney"
    ]
    ai_infra_keywords = [
        "hbm", "dram", "memory", "ãƒ¡ãƒ¢ãƒª", "gpu", "nvidia", "cuda",
        "accelerator", "ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿",
        "datacenter", "data center", "ãƒ‡ãƒ¼ã‚¿ã‚»ãƒ³ã‚¿ãƒ¼",
        "training", "inference", "æ¨è«–", "å­¦ç¿’", "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°",
        "aiå‘ã‘", "aiç”¨", "aiå¯¾å¿œ"
    ]

    ai_core_hits = sum(1 for k in ai_core_keywords if k in t)
    ai_infra_hit = has_ai_marker and any(k in t for k in ai_infra_keywords)

    # â‘  çµŒæ¸ˆRSSã¯çµŒæ¸ˆå›ºå®šï¼ˆæ ªè½ã¡é˜²æ­¢ï¼‰
    #    â€»ã“ã“ã¯é‹ç”¨ä¸Šã„ã¡ã°ã‚“å®‰å®šã™ã‚‹ã®ã§æ®‹ã™ã®ãŒãŠã™ã™ã‚
    if key == "economy":
        return "çµŒæ¸ˆ"

    # â‘¡ çµŒæ¸ˆå„ªå…ˆã¯ã€Œå¼·ã„é‡‘èèªãŒ2å€‹ä»¥ä¸Šã€ã¾ãŸã¯ã€ŒAIãƒãƒ¼ã‚«ãƒ¼+å¼·ã„é‡‘èèª1å€‹ä»¥ä¸Šã€
    #    â†’ AIéŠ˜æŸ„/AIãƒãƒ–ãƒ«ã¯çµŒæ¸ˆã«ãªã‚‹
    if finance_hits >= 2:
        return "çµŒæ¸ˆ"
    if finance_hits >= 1 and has_ai_marker:
        return "çµŒæ¸ˆ"

    # â‘¢ AIï¼šç”ŸæˆAI/LLMã‚³ã‚¢èªå½™ãŒã‚ã‚Œã°AI
    if ai_core_hits >= 1:
        return "AI"

    # â‘£ AIã‚¤ãƒ³ãƒ•ãƒ©ï¼šAIãƒãƒ¼ã‚«ãƒ¼ + ã‚¤ãƒ³ãƒ•ãƒ©èªå½™ã§AI
    if ai_infra_hit:
        return "AI"

    return "ãã®ä»–"

# ********** timestampç”Ÿæˆ **********
def format_timestamp(entry):
    """
    RSSã®pubDateã‚’JSTã® 'YYYY-MM-DD HH:MM' ã«çµ±ä¸€ã€‚
    pubDateãŒç„¡ã‘ã‚Œã°ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ç”¨ã€‚
    """
    try:
        if hasattr(entry, "published"):
            dt = feedparser._parse_date(entry.published)
        elif hasattr(entry, "updated"):
            dt = feedparser._parse_date(entry.updated)
        else:
            dt = None
    except:
        dt = None

    # pubDateå–å¾—å¤±æ•— â†’ ä»Šã®æ—¥æ™‚ã‚’ä½¿ã†
    if dt is None:
        dt_obj = datetime.now(timezone.utc)
    else:
        dt_obj = datetime(*dt[:6], tzinfo=timezone.utc)

    # JSTã¸å¤‰æ›
    jst = dt_obj.astimezone(timezone(timedelta(hours=9)))

    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    return jst.strftime("%Y-%m-%d %H:%M")

def main():
    output_items = []

    for category, info in RSS_SOURCES.items():
        #ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆã€€ã“ã“ã‹ã‚‰â†“
        print("DEBUG category key:", repr(category), "-> normalized:", repr((category or "").strip().lower()))
        #ãƒ‡ãƒãƒƒã‚°ãƒ—ãƒªãƒ³ãƒˆã€€ã“ã“ã¾ã§â†‘ã€€

        print(f"\nğŸ” [{info['source']}] RSSå–å¾—ä¸­...")

        # --- AIã‚«ãƒ†ã‚´ãƒªã¯ 2ä»¶ãƒ­ã‚¸ãƒƒã‚¯ ---
        if category == "ai":
            entries = fetch_rss_ai_multiple(info["url"], max_items=2)
        else:
            # --- ãã‚Œä»¥å¤–ã®ã‚«ãƒ†ã‚´ãƒªã¯é€šå¸¸1ä»¶ ---
            entry = fetch_rss(info["url"])
            if not entry:
                print(f"âš ï¸ {info['source']} ã®RSSãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                continue
            entries = [entry]  # â† 1ä»¶ã‚’ãƒªã‚¹ãƒˆåŒ–ã—ã¦çµ±ä¸€å‡¦ç†ã«ã™ã‚‹

        # --- entriesã®å…±é€šå‡¦ç† ---
        for entry in entries:
            title = entry.title
            link = entry.link
            description = entry.summary if hasattr(entry, "summary") else ""

            print(f"ğŸ§  [{info['source']}] è¦ç´„ä¸­...")
            summary = summarize(description, title)

            # â˜… è‹±èªè¨˜äº‹ï¼ˆVentureBeat / BBCï¼‰ã®ã¿æ—¥æœ¬èªç¿»è¨³ã‚’ã‹ã‘ã‚‹
            title_ja = title
            summary_ja = summary
            title_en = ""
            summary_en = ""

            if info["source"] in ["VentureBeat", "BBC"]:
                title_en = title
                summary_en = summary
                print(f"ğŸŒ [{info['source']}] æ—¥æœ¬èªç¿»è¨³ä¸­...")
                title_ja, summary_ja = translate_to_japanese(title_en, summary_en)
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¤å®šï¼ˆtitle + description + summary ã§åˆ¤å®šï¼‰
            category_final = classify_category(title, summary, category, description)

            timestamp = format_timestamp(entry)

            output_items.append({
                "source": info['source'],
                "title": title_ja,        # æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«
                "title_en": title_en,     # è‹±èªã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè‹±èªè¨˜äº‹ã®ã¿ã€ãã‚Œä»¥å¤–ã¯ç©ºæ–‡å­—ï¼‰
                "summary": summary_ja,    # æ—¥æœ¬èªè¦ç´„
                "summary_en": summary_en, # è‹±èªè¦ç´„ï¼ˆè‹±èªè¨˜äº‹ã®ã¿ã€ãã‚Œä»¥å¤–ã¯ç©ºæ–‡å­—ï¼‰
                "link": link,
                "category": category_final,
                "publishedAt": timestamp
            })

    with open(DATA_PATH, "w", encoding="utf-8") as f:
        json.dump(output_items, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… è¤‡æ•°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚ã¦ {os.path.basename(DATA_PATH)} ã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼")

if __name__ == "__main__":
    main()
