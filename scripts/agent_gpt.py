import os
import feedparser
import json
from openai import OpenAI
from dotenv import load_dotenv
from datetime import datetime, timezone, timedelta

# 1) .env èª­ã¿è¾¼ã¿
load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=API_KEY)

RSS_SOURCES = {
    "ai": {
        "source": "VentureBeat",
        "url": "https://venturebeat.com/category/ai/feed/"
    },
    "economy": {
        "source": "Bloomberg",
        "url": "https://www.bloomberg.com/feeds/markets/news.rss"
    },
    "world": {
        "source": "BBC",
        "url": "https://feeds.bbci.co.uk/news/world/rss.xml"
    },
    "japan_politics": {
        "source": "NHK",
        "url": "https://www3.nhk.or.jp/rss/news/cat3.xml"
    }
}

# ********** RSSå–å¾— **********
def fetch_rss(url):
    feed = feedparser.parse(url)
    # è¨˜äº‹ãŒç©ºã§ãªã„æ™‚ã ã‘1ä»¶
    return feed.entries[0] if feed.entries else None

def fetch_rss_ai_multiple(url, max_items=2):
    """
    AIã‚«ãƒ†ã‚´ãƒªå°‚ç”¨ï¼šOpenAI/ChatGPTé–¢é€£ã‚’å„ªå…ˆã—ã¤ã¤ã€
    æœ€å¤§ max_items ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¿”ã™é–¢æ•°ã€‚
    """
    feed = feedparser.parse(url)
    entries = feed.entries

    if not entries:
        return []

    keywords = ["openai", "chatgpt", "sam altman", "gpt", "large language model"]

    # --- â‘  å„ªå…ˆè¨˜äº‹ï¼ˆOpenAI/ChatGPTé–¢é€£ï¼‰ã‚’å…ˆã«å–å¾—
    priority_items = []
    normal_items = []

    for e in entries:
        text = (e.title + " " + e.get("summary", "")).lower()
        if any(k in text for k in keywords):
            priority_items.append(e)
        else:
            normal_items.append(e)

    # --- â‘¡ å„ªå…ˆ â†’ é€šå¸¸ ã®é †ã§ max_items ä»¶å–ã‚Šå‡ºã™
    combined = priority_items + normal_items
    return combined[:max_items]

# ********** titleãƒ»summaryæŠ½å‡º / 3è¡Œè¦ç´„ **********
def summarize(text, title=""):
    if not text or text.strip() == "":
        prompt = (
            "æ¬¡ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã€è¨˜äº‹ã®å†…å®¹ã‚’æ¨æ¸¬ã—ã¦3è¡Œã®è¦ç´„æ–‡ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n"
            f"ã‚¿ã‚¤ãƒˆãƒ«: {title}\n"
        )
    else:
        prompt = f"æ¬¡ã®è¨˜äº‹ã‚’3è¡Œã§è¦ç´„ã—ã¦ãã ã•ã„ï¼š\n{text}"
    
    res = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )

    content = res.choices[0].message.content

    # content ãŒæ–‡å­—åˆ—ã®å ´åˆ
    if isinstance(content, str):
        return content

    # content ãŒé…åˆ—ï¼ˆMessageContentï¼‰ã§è¿”ã‚‹å ´åˆ
    if isinstance(content, list) and len(content) > 0:
        first = content[0]
        # textå±æ€§ã‚’æŒã¤ã‚¿ã‚¤ãƒ—
        if hasattr(first, "text"):
            return first.text
        # ä¸‡ãŒä¸€ text ãŒãªãã¦ã‚‚ string_value ãŒã‚ã‚‹
        if hasattr(first, "string_value"):
            return first.string_value

    # ãã‚Œã§ã‚‚ãƒ€ãƒ¡ãªã‚‰ã€ã¨ã‚Šã‚ãˆãšæ–‡å­—åˆ—åŒ–ã—ã¦è¿”ã™
    return str(content)

 # ********** ã‚«ãƒ†ã‚´ãƒªåˆ¤å®š **********
def classify_category(title, summary, initial_category):
    text = (title + " " + summary).lower()

    # --- AI ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ---
    ai_keywords = [
        "ai", "artificial intelligence", "gpt", "chatgpt",
        "openai", "neural", "model", "llm", "gemini",
        "anthropic", "deepseek", "ç”Ÿæˆai", "æ©Ÿæ¢°å­¦ç¿’"
    ]

    # --- çµŒæ¸ˆ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ ---
    economy_keywords = [
        "stock", "market", "shares", "inflation", "finance",
        "çµŒæ¸ˆ", "ä¼æ¥­", "æ ª", "æ™¯æ°—", "è³ƒé‡‘", "è³‡é‡‘", "é‡‘åˆ©"
    ]

    # --- â‘  AIåˆ¤å®š ---
    if any(k in text for k in ai_keywords):
        return "AI"

    # --- â‘¡ çµŒæ¸ˆåˆ¤å®š ---
    if any(k in text for k in economy_keywords):
        return "çµŒæ¸ˆ"

    # --- â‘¢ ã©ã¡ã‚‰ã§ã‚‚ãªã„å ´åˆ â†’ ãã®ä»– ---
    return "ãã®ä»–"

# ********** timestampç”Ÿæˆ **********
def format_timestamp(entry):
    """
    RSSã®pubDateã‚’JSTã® 'YYYY-MM-DD HH:MM' ã«çµ±ä¸€ã€‚
    pubDateãŒç„¡ã‘ã‚Œã°ç¾åœ¨æ™‚åˆ»ã‚’ä½¿ç”¨ã€‚
    """
    try:
        if hasattr(entry, "published"):
            dt = feedparser._parse_date(entry.published)
        elif hasattr(entry, "updated"):
            dt = feedparser._parse_date(entry.updated)
        else:
            dt = None
    except:
        dt = None

    # pubDateå–å¾—å¤±æ•— â†’ ä»Šã®æ—¥æ™‚ã‚’ä½¿ã†
    if dt is None:
        dt_obj = datetime.now(timezone.utc)
    else:
        dt_obj = datetime(*dt[:6], tzinfo=timezone.utc)

    # JSTã¸å¤‰æ›
    jst = dt_obj.astimezone(timezone(timedelta(hours=9)))

    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    return jst.strftime("%Y-%m-%d %H:%M")

def main():
    output_items = []

    for category, info in RSS_SOURCES.items():
        print(f"\nğŸ” [{info['source']}] RSSå–å¾—ä¸­...")

        # --- AIã‚«ãƒ†ã‚´ãƒªã¯ 2ä»¶ãƒ­ã‚¸ãƒƒã‚¯ ---
        if category == "ai":
            entries = fetch_rss_ai_multiple(info["url"], max_items=2)

        else:
            # --- ãã‚Œä»¥å¤–ã®ã‚«ãƒ†ã‚´ãƒªã¯é€šå¸¸1ä»¶ ---
            entry = fetch_rss(info["url"])
            if not entry:
                print(f"âš ï¸ {info['source']} ã®RSSãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                continue
            entries = [entry]  # â† 1ä»¶ã‚’ãƒªã‚¹ãƒˆåŒ–ã—ã¦çµ±ä¸€å‡¦ç†ã«ã™ã‚‹

        # --- entriesã®å…±é€šå‡¦ç† ---
        for entry in entries:
            title = entry.title
            link = entry.link
            description = entry.summary if hasattr(entry, "summary") else ""

            print(f"ğŸ§  [{info['source']}] è¦ç´„ä¸­...")
            summary = summarize(description, title)

            category_final = classify_category(title, summary, category)

            timestamp = format_timestamp(entry)

            output_items.append({
                "source": info['source'],
                "title": title,
                "summary": summary,
                "link": link,
                "category": category_final
            })

    output = {"news": output_items}

    with open("summary.json", "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print("\nâœ… è¤‡æ•°ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¾ã¨ã‚ã¦ summary.json ã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼")

if __name__ == "__main__":
    main()

